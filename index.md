## Overview of the competition

Today, evaluation of dialogue agents is severely limited by the absence of accurate formal metrics. Existing statistical measures such as perplexity, BLEU, recall and others are not sufficiently correlated with human evaluation [1]. Blind assessment of communication quality by humans is a straightforward solution famously proposed by Alan Turing as a test for machine intelligence [2]. Unfortunately, human assessment is time and resource consuming. Here we propose to crowdsource evaluation of dialogue systems in the form of a live competition. Participants of the competition, as well as volunteers, will be asked to perform a blind evaluation of a discussion about a news/wikipedia article with either a bot or a human peer. As a result we expect to have two outcomes: (1) a measure of quality of state of the art dialogue systems compared to human level, and (2) an open source dataset collected from evaluated dialogs.

[1] Liu, Chia-Wei, et al. "How NOT to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation." arXiv preprint arXiv:1603.08023 (2016).

[2] Turing, Alan M. "Computing machinery and intelligence." Mind 59.236 (1950): 433-460.
